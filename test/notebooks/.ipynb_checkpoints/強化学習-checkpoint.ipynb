{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b20aac92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.4 0.1]\n",
      " [0.  0.  0. ]\n",
      " [0.  0.  0. ]]\n",
      "[[0.5  0.4  0.1 ]\n",
      " [0.25 0.5  0.25]\n",
      " [0.   0.   0.  ]]\n",
      "[[0.5  0.4  0.1 ]\n",
      " [0.25 0.5  0.25]\n",
      " [0.5  0.   0.5 ]]\n",
      "A:  [[ 0.525  -0.38   -0.095 ]\n",
      " [-0.2375  0.525  -0.2375]\n",
      " [-0.475   0.      0.525 ]]\n",
      "B(inverse of A):  [[8.95977245 6.48516863 4.55505892]\n",
      " [7.72043885 7.49288907 4.78667208]\n",
      " [8.10646079 5.86753352 6.02600569]]\n",
      "values under current policy:  [17.30902072 17.51117432 16.13673304]\n",
      "\n",
      "Step Statistics\n",
      "step: 0 \n",
      " value: [17.30902072 17.51117432 16.13673304] \n",
      " policy: [0.5, 0.5, 0.5] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#各種モジュールのインポート\n",
    "import numpy as np\n",
    "import copy\n",
    "# POLICY - how the agent decides to take an action\n",
    "# probability of taking one branch\n",
    "# probability of taking the other branch is (1-p[i])\n",
    "# there is a probability to take a branch for each state(Home, Office, Bar)\n",
    "# the branch taken determines the next state. This is for state trasnition.\n",
    "#MDPの設定\n",
    "p = [0.8, 0.5, 1.0]\n",
    "\n",
    "# the reduction rate used for each element in the series that make up the\n",
    "# total reward sum.\n",
    "#割引率の設定\n",
    "gamma = 0.95\n",
    "\n",
    "#(Home: S0, Office: S1, Bar: S2) (Move: A0, Stay: A1)\n",
    "# initialized reward values.\n",
    "# an entry for each state, state prime, and action combination\n",
    "# or in other words and entry/reward for each state transition and action\n",
    "# Look at the state transition diagram\n",
    "#報酬期待値の設定\n",
    "r = np.zeros((3,3,2))\n",
    "r[0,1,0]=1.0 #Home to Office with Move\n",
    "r[0,2,0]=2.0 #Home to Bar with Move\n",
    "r[0,0,1]=0.0 #Home to Home with Stay\n",
    "\n",
    "r[1,0,0]=1.0\n",
    "r[1,2,0]=2.0\n",
    "r[1,1,1]=1.0\n",
    "\n",
    "r[2,0,0]=1.0\n",
    "r[2,1,0]=1.0\n",
    "r[2,2,1]=-1.0\n",
    "\n",
    "# value function initial values\n",
    "#価値関数の初期化\n",
    "v = [0,0,0]\n",
    "v_prev = copy.copy(v)\n",
    "\n",
    "#pg.68\n",
    "# action value function intitial values\n",
    "# initialize policy distrobution\n",
    "# 3 for each state? 2 for each action?\n",
    "q = np.zeros((3,2))\n",
    "# policy distribution initial values\n",
    "#方策分布の初期化\n",
    "pi = [0.5,0.5,0.5]\n",
    "\n",
    "#方策評価関数の定義\n",
    "def policy_estimator(pi, p, r, gamma):\n",
    "    #pg. 75\n",
    "    #初期化\n",
    "    R = [0,0,0]\n",
    "    P = np.zeros((3,3))\n",
    "    A = np.zeros((3,3))\n",
    "    \n",
    "    # for each state\n",
    "    for i in range(3):\n",
    "        #compute the matrix entries for state transition\n",
    "        #状態遷移行列の計算\n",
    "        P[i,i]       = 1-pi[i]\n",
    "        P[i,(i+1)%3] = p[i]*pi[i]\n",
    "        P[i,(i+2)%3] = (1-p[i])*pi[i]\n",
    "        print(P)\n",
    "        \n",
    "        #compute the matrix entries for rewards\n",
    "        #報酬ベクトルの計算\n",
    "        term0a = (p[i]   ) * r[i,(i+1)%3, 0]\n",
    "        term0b = (1-pi[i]) * r[i,(i+2)%3,0]\n",
    "        term1  = (1-pi[i]) * r[i,i,1]\n",
    "        \n",
    "        term0 = (term0a + term0b)\n",
    "        \n",
    "    \n",
    "        R[i] = pi[i] * term0 + term1\n",
    "    \n",
    "    # bellman equation derived stuff\n",
    "    # 3x3identity matrix sub gamma(reduction factor)*P (transition probability matrix influenced by the current policy)\n",
    "    #行列計算によるベルマン方程式の求解\n",
    "    A=np.eye(3)-gamma*P\n",
    "    B=np.linalg.inv(A)\n",
    "    \n",
    "    print(\"A: \", A)\n",
    "    print(\"B(inverse of A): \", B) #inverse of A\n",
    "    \n",
    "    v_sol=np.dot(B,R)\n",
    "    return v_sol\n",
    "\n",
    "\n",
    "# 方策反復法の計算\n",
    "for step in range(1):\n",
    "    v = policy_estimator(pi, p, r, gamma)\n",
    "    print(\"values under current policy: \", v)\n",
    "    \n",
    "    #価値関数vが前\n",
    "    if np.min(v-v_prev) <= 0:\n",
    "        break\n",
    "    \n",
    "    #現ステップの価値関数と方策を表示\n",
    "    print('\\nStep Statistics\\nstep:', step,'\\n', 'value:', v, '\\n','policy:', pi,'\\n')\n",
    "    \n",
    "    #方策改善ステップ\n",
    "    for i in range(3):\n",
    "        t0 = p[i]*(r[i,(i+1)%3,0] + gamma*v[(i+1)%3])\n",
    "        t1 = (1-p[i])*(r[i,(i+2)%3, 0] + gamma*v[(i+2)%3])\n",
    "\n",
    "        q[i, 0] = t0\n",
    "        q[i,1] = r[i,i,1]+gamma*v[i]\n",
    "\n",
    "        #行動価値関数のもとでgreedyに方策を改善\n",
    "        if q[i,0]>q[i,1]:\n",
    "            pi[i] = 1\n",
    "        elif q[i,0] == q[i,1]:\n",
    "            pi[i] = 0.5\n",
    "        else:\n",
    "            pi[i] = 0\n",
    "    \n",
    "    #現ステップの価値関数の記録\n",
    "    v_prev = copy.copy(v)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df631eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ebc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7e459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
